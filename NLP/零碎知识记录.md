
## 文本特征提取模型

* 参考文章[NLP之词袋模型和TF-IDF模型](http://www.freebuf.com/column/167084.html)

* 词集模型: 单词构成的集合，集合自然每个元素都只有一个，也即词集中的每个单词都只有一个

* 词袋模型: 在词集的基础上如果一个单词在文档中出现不止一次，统计其出现的次数（频数）

* TF-IDF模型: TF-IDF是一种统计方法，用以评估某一字词对于一个文件集或一个语料库的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。


## 临时记录

jieba分词？？ gensim(Word2Vec)文本相似性？？

Word2Vec:CBOW(Continuous Bag-Of-Words，即连续的词袋模型)和Skip-Gram 两种
CBOW模型能够根据输入周围n-1个词来预测出这个词本身，而Skip-gram模型能够根据词本身来预测周围有哪些词。也就是说，CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量，而Skip-gram模型的输入是词A本身，输出是词A周围的n个单词的词向量。Word2Vec最常用的开源实现之一就是gensim，网址为:http://radimrehurek.com/gensim/

词袋模型、tfidf模型等产生的是词向量，word2Vec也是用来产生词向量，什么区别？
词袋模型 / word2Vec的区别

秒懂词向量Word2Vec的本质


Doc2Vec:Distributed Memory (DM) 和Distributed Bag of Words (DBOW)


http://www.52nlp.cn/tag/gensim
https://zhuanlan.zhihu.com/p/26306795


word2vec,doc2vec


## 个人Notes

* sklearn中的CountVectorizer()和TfidfTransformer()最终得到的是基于词袋模型和tfidf模型对文本的向量表示;gensim.corpora.directionary.doc2bow()也能够得到tf-idf值，但是结果表示方式不同，且gensim.similarities针对gensim.corpora.directionary.doc2bow()所得的tfidf形式便于进行相似度的计算，具体见**他人代码学习/document_similarity_cal.py**。相比之下，如果是提供机器学习时文本的向量表示，用CountVectorizer()和TfidfTransformer()较为便利。

* word2vec建模所得的Model是针对词的，model[word]可得到该词在每篇文章中对应的概率。其他用法详见**wordvector_test.py**。


word embedding & word2vec???