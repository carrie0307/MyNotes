{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17eecd73710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch seq2seq考虑mask的loss计算\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/masked_cross_entropy.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "torch.manual_seed(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    # seq_range = torch.range(0, max_len - 1).long()\n",
    "    # 注意这里的修改\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意要修改的地方torch.range()：由于版本问题，torch.range()用法将不被支持，应当改为torch.arange()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 注意torch.range()与torch.arange()区别\n",
    "a = torch.range(0,3)\n",
    "print (a)\n",
    "b = torch.arange(0,3)\n",
    "print (b)\n",
    "# 根据区别，将原代码中的seq_range = torch.range(0, max_len - 1).long()改为seq_range = torch.arange(0, max_len ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, target, length):\n",
    "    \n",
    "    # length = Variable(torch.LongTensor(length)).cuda()\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat, dim=-1) # 先做softmax,再分别取log\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat) # 用gather函数选出实际类别对应的预测概率(愿意参照https://blog.csdn.net/qq_22210253/article/details/85229988)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6427, 0.4695, 0.8888, 0.7078],\n",
       "         [0.2212, 0.2054, 0.7708, 0.3889],\n",
       "         [0.3415, 0.7100, 0.3211, 0.7718],\n",
       "         [0.7857, 0.7259, 0.7164, 0.8696]],\n",
       "\n",
       "        [[0.8742, 0.9993, 0.8661, 0.0262],\n",
       "         [0.9472, 0.0056, 0.5668, 0.9110],\n",
       "         [0.3902, 0.2152, 0.2788, 0.0949],\n",
       "         [0.6346, 0.7060, 0.5457, 0.6759]],\n",
       "\n",
       "        [[0.9141, 0.7632, 0.8881, 0.0853],\n",
       "         [0.4445, 0.4018, 0.6818, 0.7057],\n",
       "         [0.5727, 0.5517, 0.6963, 0.9769],\n",
       "         [0.1144, 0.6929, 0.6979, 0.2920]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.rand((3,4,4))  # batch=3,max_len=4,num_classes=3 (实际类别数量是3,包含一个padding=0)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 3],\n",
       "        [2, 3, 3, 0],\n",
       "        [2, 1, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[1,2,3,3],[2,3,3,0],[2,1,0,0]]) # 令0做tag_padding,则如果实际由三类，就需要有0，1，2，3四个tag_id (也可以是其他的做padding)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = torch.tensor([4,3,2])\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 1, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先看sequence_mask(sequence_length, max_len=None)函数的效果\n",
    "mask = sequence_mask(sequence_length=length, max_len=target.size(1)) #max_len是当前batch中的最大长度\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，sequence_mask输出为1的部分即是实际长度的部分，可以用得到的mask矩阵来做计算loss时的屏蔽处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3153)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算loss\n",
    "loss = masked_cross_entropy(logits, target, length)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另外一种想法是:既然pandding部分的误差会通过与mask相乘被过滤掉，那么不妨令target中的padding为0，这样不用按照num_classes+1来处理\n",
    "# 在seq2seq预测结果是words时，一般都包括了padding,unk等，因此可以用上面的方法处理；\n",
    "# 如果预测的是pos,个人认为应该用下面这种方法，保证最后一个全连接层输出的维度与实际的num_classes一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3551, 0.9598, 0.6522],\n",
       "         [0.2716, 0.0676, 0.9010],\n",
       "         [0.6095, 0.1420, 0.4506],\n",
       "         [0.1210, 0.4030, 0.0129]],\n",
       "\n",
       "        [[0.0405, 0.2025, 0.0784],\n",
       "         [0.6600, 0.3458, 0.9125],\n",
       "         [0.5765, 0.0901, 0.7497],\n",
       "         [0.4754, 0.6096, 0.6145]],\n",
       "\n",
       "        [[0.0646, 0.5801, 0.4453],\n",
       "         [0.9615, 0.2805, 0.8449],\n",
       "         [0.5224, 0.2518, 0.3558],\n",
       "         [0.8679, 0.1321, 0.9071]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_logits = torch.rand((3,4,3))  # batch=3,max_len=4,num_classes=3 \n",
    "new_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 2],\n",
       "        [1, 2, 2, 3],\n",
       "        [1, 0, 3, 3]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_target = torch.tensor([[0,1,2,2],[1,2,2,3],[1,0,3,3]]) # 3代表padding占位\n",
    "new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_length = torch.tensor([4,3,2])\n",
    "new_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_masked_cross_entropy(logits, target, length):\n",
    "    \n",
    "    # length = Variable(torch.LongTensor(length)).cuda()\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = functional.log_softmax(logits_flat, dim=-1) # 这里添加了维度\n",
    "    target_mask = target * mask.long() # 先把padding位屏蔽为0，使得gather时不会越界\n",
    "    print (\"target_mask:\", target_mask)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_mask_flat = target_mask.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_mask_flat) # \n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size()) \n",
    "    losses = losses * mask.float()\n",
    "    print (\"losses: \", losses)\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_mask: tensor([[0, 1, 2, 2],\n",
      "        [1, 2, 2, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "losses:  tensor([[1.4295, 1.5101, 1.0670, 1.2785],\n",
      "        [1.0057, 0.8520, 0.8578, 0.0000],\n",
      "        [0.9047, 0.8738, 0.0000, 0.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0866)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss = new_masked_cross_entropy(new_logits, new_target, length)\n",
    "new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3551, 0.9598, 0.6522],\n",
       "        [0.2716, 0.0676, 0.9010],\n",
       "        [0.6095, 0.1420, 0.4506],\n",
       "        [0.1210, 0.4030, 0.0129],\n",
       "        [0.0405, 0.2025, 0.0784],\n",
       "        [0.6600, 0.3458, 0.9125],\n",
       "        [0.5765, 0.0901, 0.7497],\n",
       "        [0.4754, 0.6096, 0.6145],\n",
       "        [0.0646, 0.5801, 0.4453],\n",
       "        [0.9615, 0.2805, 0.8449],\n",
       "        [0.5224, 0.2518, 0.3558],\n",
       "        [0.8679, 0.1321, 0.9071]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证\n",
    "val_logits = new_logits.view(-1, new_logits.size(-1))\n",
    "val_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_logits = torch.tensor([[0.3551, 0.9598, 0.6522],\n",
    "        [0.2716, 0.0676, 0.9010],\n",
    "        [0.6095, 0.1420, 0.4506],\n",
    "        [0.1210, 0.4030, 0.0129],\n",
    "        [0.0405, 0.2025, 0.0784],\n",
    "        [0.6600, 0.3458, 0.9125],\n",
    "        [0.5765, 0.0901, 0.7497],\n",
    "        [0.0646, 0.5801, 0.4453],\n",
    "        [0.9615, 0.2805, 0.8449]]) # 删掉padding部分的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets = torch.tensor([0, 1, 2, 2, 1, 2, 2, 1, 0]) # 去掉padding后的target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0866)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss_value = loss(val_logits, val_targets)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到[Out]104和[Out]108的结果是一致的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
