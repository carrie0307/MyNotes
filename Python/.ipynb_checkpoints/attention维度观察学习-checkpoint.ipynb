{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "# attention维度观察与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_encoder_outputs = torch.randint(1,9,(3,2,4)) # len=3,batch_size=2, hidden_dim=4\n",
    "init_curr_hidden = torch.randint(1,9,(1,2,4)) # len=1,batch_size=2, hidden_dim=4  len=1是只允许了当前一个时间步的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7., 4., 3., 4.],\n",
       "         [7., 1., 2., 8.]],\n",
       "\n",
       "        [[1., 1., 1., 4.],\n",
       "         [8., 2., 2., 3.]],\n",
       "\n",
       "        [[8., 3., 4., 4.],\n",
       "         [2., 3., 2., 6.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里测试batch_first=False\n",
    "encoder_outputs = init_encoder_outputs.data.float()\n",
    "encoder_outputs # [len=3,batch_size=2,hidden_Dim=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7., 5., 1., 2.],\n",
       "         [2., 8., 2., 1.]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_hidden = init_curr_hidden.data.float() # len=1,batch_size=2, hidden_dim=4  len=1是只允许了当前一个时间步的意思\n",
    "curr_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80., 34.],\n",
       "        [21., 39.],\n",
       "        [83., 38.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot方法\n",
    "dot_sim = torch.sum(encoder_outputs * curr_hidden, dim=2)\n",
    "dot_sim # batch_size, max_len (dot_sim[i][j]代表第i个样本decoder时，当前curr_hidden与encoder_outputs的第j个时间步hidden的相似性)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_sim.shape # [len, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意这一步代码\n",
    "dot_sim = dot_sim.t()\n",
    "dot_sim.shape # [batch_size, len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.7426e-02, 1.1289e-27, 9.5257e-01]],\n",
       "\n",
       "        [[4.9017e-03, 7.2748e-01, 2.6762e-01]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_sim = dot_sim.data.float() # 转化为float,便于进行softmax\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "att_weights = softmax(dot_sim).unsqueeze(1)\n",
    "att_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_weights.shape # batch_size, 1, len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs = encoder_outputs.data.float()\n",
    "encoder_outputs.transpose(0, 1).shape # batch, len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, 1, max_len] [batch_size, len, hidden_dim] \n",
    "c = att_weights.bmm(encoder_outputs.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.9526, 3.0474, 3.9526, 4.0000]],\n",
       "\n",
       "        [[6.3894, 2.2627, 2.0000, 3.8274]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7., 4., 3., 4.],\n",
       "         [1., 1., 1., 4.],\n",
       "         [8., 3., 4., 4.]],\n",
       "\n",
       "        [[7., 1., 2., 8.],\n",
       "         [8., 2., 2., 3.],\n",
       "         [2., 3., 2., 6.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里测试batch_first=True的情况\n",
    "encoder_outputs = init_encoder_outputs.transpose(0,1)\n",
    "encoder_outputs = encoder_output.data.float()\n",
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7., 5., 1., 2.]],\n",
       "\n",
       "        [[2., 8., 2., 1.]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_hidden = init_curr_hidden.transpose(0,1)\n",
    "curr_hidden = curr_hidden.data.float() # len=1,batch_size=2, hidden_dim=4  len=1是只允许了当前一个时间步的意思\n",
    "curr_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[80., 21., 83.],\n",
       "        [34., 39., 38.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot方法\n",
    "dot_sim = torch.sum(encoder_outputs * curr_hidden, dim=2)\n",
    "dot_sim # batch_size, max_len (dot_sim[i][j]代表第i个样本decoder时，当前curr_hidden与encoder_outputs的第j个时间步hidden的相似性)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_sim.shape # 这里和batch_first=False进行dot_sim转置后相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.7426e-02, 1.1289e-27, 9.5257e-01]],\n",
       "\n",
       "        [[4.9017e-03, 7.2748e-01, 2.6762e-01]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "att_weights = softmax(dot_sim).unsqueeze(1)\n",
    "att_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.9526, 3.0474, 3.9526, 4.0000]],\n",
       "\n",
       "        [[6.3894, 2.2627, 2.0000, 3.8274]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attn_weights: [batch_size, 1, max_len] encoder_outputs: [batch_size, len, hidden_dim] \n",
    "c = att_weights.bmm(encoder_outputs) # attention权重和encoder_outputs的hd对应相乘\n",
    "c # c就是decoder当前隐状态经过attention计算后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape # [batch_size, 1, hidden_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续，对curr_hidden和c都进行sequeese(1)操作使二者的shape=(batch_size, hidden_dim),之后就可以进行concat操作(torch.cat((rnn_output, context), 1),注意这样处理后第二维度变为hidden_dim*2)，操作后即可进行输出的映射，具体见https://github.com/carrie0307/pytorch_practise/blob/a5b6501052bd0a4b1be369b49db14daa737d6b96/chatbot/model.py#L116 (这里虽然是batch_first=False的操作，但是最终得到的c与batch_first=True时相同，因此这部分的代码可以通用)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "总结:\n",
    "    1. batch_first与否对attention结果计算没有影响,区别在于batch_first=False时，计算dot_sim进行softmax操作前，需要对dot_sim的batch_size和max_len进行维度转置，bmm之前需要对encoder_output的sbatch_size和max_len进行维度转置，而在batch_first=True时则不需要；\n",
    "    2. 为了bmm的进行，注意进行softmax之前对dot_sim进行unsqueese(1)操作；\n",
    "    3. 再次研究chatbot代码发现，添加了attention结果的hidden用于做预测(需要进行self.out的操作)， 输送给下一时间的隐状态是原始的当前hidden_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
